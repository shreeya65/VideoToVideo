{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "uploaded = None\n",
        "resize_to_720p = False\n",
        "\n",
        "def upload_video():\n",
        "  global uploaded\n",
        "  global video_path\n",
        "  uploaded = files.upload()\n",
        "  for filename in uploaded.keys():\n",
        "    print(f'Uploaded {filename}')\n",
        "    if resize_to_720p:\n",
        "        filename = resize_video(filename)\n",
        "    video_path = filename\n",
        "    return filename\n",
        "\n",
        "\n",
        "def resize_video(filename):\n",
        "    output_filename = f\"resized_{filename}\"\n",
        "    cmd = f\"ffmpeg -i {filename} -vf scale=-1:720 {output_filename}\"\n",
        "    subprocess.run(cmd, shell=True)\n",
        "    print(f'Resized video saved as {output_filename}')\n",
        "    return output_filename\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "button = widgets.Button(description=\"Upload Video\")\n",
        "checkbox = widgets.Checkbox(value=False, description='Resize to 720p (better results)')\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_button_clicked(b):\n",
        "  with output:\n",
        "    global video_path\n",
        "    global resize_to_720p\n",
        "    resize_to_720p = checkbox.value\n",
        "    video_path = upload_video()\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "display(checkbox, button, output)"
      ],
      "metadata": {
        "id": "74LNffUNz5uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awo7TBeb30uc",
        "outputId": "f8fd2d4f-fe7c-41bd-823b-0b4b6205e667"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-xefwia61\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-xefwia61\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Ensure video_path variable exists\n",
        "if 'video_path' in globals() and video_path is not None:\n",
        "    ffmpeg_command = f\"ffmpeg -i '{video_path}' -acodec pcm_s24le -ar 48000 -q:a 0 -map a -y 'output_audio.wav'\"\n",
        "    subprocess.run(ffmpeg_command, shell=True)\n",
        "else:\n",
        "    print(\"No video uploaded. Please upload a video first.\")\n",
        "\n",
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"output_audio.wav\")\n",
        "\n",
        "whisper_text = result[\"text\"]\n",
        "whisper_language = result['language']\n",
        "\n",
        "print(\"Audio text:\", whisper_text)"
      ],
      "metadata": {
        "id": "ZonBhW8_4BQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "id": "WGgRNwBg4qIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_language = \"Hindi\" # @param [\"English\",\"Spanish\",\"French\",\"German\",\"Italian\",\"Portuguese\",\"Polish\",\"Turkish\",\"Russian\",\"Dutch\",\"Czech\",\"Arabic\",\"Chinese (Simplified)\",\"Hindi\"]\n",
        "\n",
        "# Mapping between full names and ISO 639-1 codes\n",
        "language_mapping = {\n",
        "    'English': 'en',\n",
        "    'Spanish': 'es',\n",
        "    'French': 'fr',\n",
        "    'German': 'de',\n",
        "    'Italian': 'it',\n",
        "    'Portuguese': 'pt',\n",
        "    'Polish': 'pl',\n",
        "    'Turkish': 'tr',\n",
        "    'Russian': 'ru',\n",
        "    'Dutch': 'nl',\n",
        "    'Czech': 'cs',\n",
        "    'Arabic': 'ar',\n",
        "    'Chinese (Simplified)': 'zh-cn',\n",
        "    'Hindi' : 'hi'\n",
        "}\n",
        "\n",
        "target_language_code = language_mapping[target_language]\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "translated_text = translator.translate(whisper_text, src=whisper_language, dest=target_language_code).text\n",
        "print(\"Translated text:\", translated_text)"
      ],
      "metadata": {
        "id": "ctKyqVQ04mjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "pEcyD1yZ4uP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from TTS.api import TTS\n",
        "import torch\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "\n",
        "# Use CPU if CUDA is not available\n",
        "device = \"cpu\"  # Ensure the device is set to CPU\n",
        "\n",
        "# Load the TTS model with gpu=False to make sure it doesn't attempt to use the GPU\n",
        "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=False).to(device)\n",
        "\n",
        "\n",
        "#generate audio\n",
        "tts.tts_to_file(translated_text,\n",
        "    speaker_wav='output_audio.wav',\n",
        "    file_path=\"output_synth.wav\",\n",
        "    language=target_language_code\n",
        ")\n",
        "audio_widget = Audio(filename=\"output_synth.wav\", autoplay=False)\n",
        "display(audio_widget)"
      ],
      "metadata": {
        "id": "bMXReXNi4y0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpFCh1zczoFC"
      },
      "outputs": [],
      "source": [
        "from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_audioclips\n",
        "from moviepy.audio.AudioClip import AudioArrayClip\n",
        "import numpy as np\n",
        "\n",
        "def merge_audio_with_video(video_path, audio_path, output_path, audio_delay=0):\n",
        "    try:\n",
        "        # Load the video file\n",
        "        video = VideoFileClip(video_path)\n",
        "\n",
        "        # Load the translated audio file\n",
        "        audio = AudioFileClip(audio_path)\n",
        "\n",
        "        # Apply audio delay\n",
        "        if audio_delay > 0:\n",
        "            # Create a silent audio clip\n",
        "            sr = 44100  # Sample rate\n",
        "            silence_duration = audio_delay  # In seconds\n",
        "            silence = AudioArrayClip(\n",
        "                np.zeros((int(sr * silence_duration), audio.nchannels)),\n",
        "                fps=sr\n",
        "            )\n",
        "\n",
        "            # Combine silence and audio\n",
        "            audio = concatenate_audioclips([silence, audio])\n",
        "        elif audio_delay < 0:\n",
        "            # Trim the audio to remove delay (ensure audio length > abs(delay))\n",
        "            start_time = abs(audio_delay)\n",
        "            if start_time < audio.duration:\n",
        "                audio = audio.subclip(start_time)\n",
        "            else:\n",
        "                raise ValueError(\"Audio delay exceeds the audio's length.\")\n",
        "\n",
        "        # Set the audio of the video to the adjusted audio\n",
        "        video = video.set_audio(audio)\n",
        "\n",
        "        # Write the final video to a file\n",
        "        video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
        "\n",
        "        print(f\"Video with translated audio saved at {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# File paths\n",
        "video_file = \"/content/trumph.mp4\"  # Replace with your video file path\n",
        "audio_file = \"/content/output_synth.wav\"  # Replace with your translated audio file path\n",
        "output_file = \"final_video123.mp4\"\n",
        "\n",
        "# Set the audio delay (positive to add delay, negative to trim)\n",
        "audio_delay = 1  # Example: 5 seconds delay\n",
        "merge_audio_with_video(video_file, audio_file, output_file, audio_delay)"
      ]
    }
  ]
}